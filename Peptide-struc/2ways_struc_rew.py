# -*- coding: utf-8 -*-
"""2ways_rewiring.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G85IK5EecYUEmcilTbdL6ollQuV0Lcdu
"""

# -*- coding: utf-8 -*-
"""Att_rewire.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18NVaEIRde7UdfEZSZRKmx-w56hgQuTml
"""

# Install required packages.
import os
import torch
os.environ['TORCH'] = torch.__version__
print(torch.__version__)

# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html
# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html
# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git
# !pip install einops
# !pip install wandb

### Attention-based Rewiring
## In this notebook we plan to rewire the input graph based on the attention weights from latent to output.
# Install required packages.
import os
# os.environ['TORCH'] = torch.__version__
# print(torch.__version__)

import torch
from torch_geometric.datasets import TUDataset, LRGBDataset
import os.path as osp
import torch_geometric.transforms as T
import wandb
from torch.utils.data import random_split
from torch_geometric.loader import DataLoader
import math
from torch.nn import Linear
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, SAGEConv, GATConv, GINConv,global_mean_pool, ChebConv,global_add_pool
#from torch_sparse import SparseTensor
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
import torch.nn as nn
import argparse
import random
import torch_geometric
from torch_geometric.utils import dropout_adj,dense_to_sparse
from torch_geometric.utils import to_dense_adj,dense_to_sparse #dropout_adj,

parser = argparse.ArgumentParser()
parser.add_argument('--comp', type=int, default=6, help='Latent Bottleneck')
parser.add_argument('--hidden', type=int, default=300, help='Latent Dimension')
parser.add_argument('--maxAtt', type=int, default=8, help='Max attention')
parser.add_argument('--seed', type=int, default=12345, help='Latent Dimension')
parser.add_argument('--batch_size', type=int, default=128, help='Latent Dimension')
parser.add_argument('--k', type=int, default=16, help='number of vectors')
parser.add_argument('--laplace_RW', type=bool, default=True, help='Use laplacian PE')
parser.add_argument('--laplace', type=bool, default=False, help='Use laplacian PE')
parser.add_argument('--FA', type=bool, default=False, help='Use FA Layer')
parser.add_argument('--learnable', type=bool, default=False, help='Use FA Layer')

parser.add_argument('--use_graph', type=bool, default=True, help='Use graph infos')
parser.add_argument('--patches', type=bool, default=True, help='Use graph infos')
parser.add_argument('--pretrain', type=bool, default=False, help='Use graph infos')
parser.add_argument('--lamb', type=float, default=0.5, help='Regularizer')
parser.add_argument('--use_weights', type=bool, default=False, help='Use graph infos')
parser.add_argument('--gConv', type=str, default='GAT', help='graph conv between latents')
parser.add_argument('--lr', type=float, default=0.0005, help='Learning rate')
parser.add_argument('--webdata', type=str, default='uqweyrweqhueir', help='which data')
args = parser.parse_args([])

from torch_geometric.transforms import AddLaplacianEigenvectorPE
check=AddLaplacianEigenvectorPE(k=args.k)
torch.manual_seed(args.seed)

from torch_geometric.transforms import AddLaplacianEigenvectorPE, AddRandomWalkPE
def random_walk(A, n_iter):
    # Geometric diffusion features with Random Walk
    Dinv = A.sum(dim=-1).clamp(min=1).pow(-1).unsqueeze(-1)  # D^-1
    RW = A * Dinv
    M = RW
    M_power = M
    # Iterate
    PE = [torch.diagonal(M)]
    for _ in range(n_iter-1):
        M_power = torch.matmul(M_power, M)
        PE.append(torch.diagonal(M_power))
    PE = torch.stack(PE, dim=-1)
    return PE



def RWSE(edge_index, pos_enc_dim, num_nodes):
    """
        Initializing positional encoding with RWSE
    """
    if edge_index.size(-1) == 0:
        PE = torch.zeros(num_nodes, pos_enc_dim)
    else:
        A = torch_geometric.utils.to_dense_adj(
            edge_index, max_num_nodes=num_nodes)[0]
        PE = random_walk(A, pos_enc_dim)
    return PE


def falayer(datasetz):
  outs=[]
  ins=[]
  dataset2=[]
  for p in range(len(datasetz)):
    try:
      tempo=datasetz[p]
      tempo['edge_index']= dense_to_sparse(torch.ones(tempo.x.shape[0],tempo.x.shape[0]))[0]
      dataset2.append(tempo)
      ins.append(p)
    except:
      print(p)
      outs.append(p)
  return dataset2



from torch_geometric.nn import global_add_pool

BN = True


class Identity(nn.Module):
    def __init__(self, *args, **kwargs):
        super(Identity, self).__init__()

    def forward(self, input):
        return input

    def reset_parameters(self):
        pass


from torch_scatter import scatter
class MLP(nn.Module):
    def __init__(self, nin, nout, nlayer=2, with_final_activation=True, with_norm=True, bias=True):
        super().__init__()
        n_hid = nin
        self.layers = nn.ModuleList([nn.Linear(nin if i == 0 else n_hid,
                                     n_hid if i < nlayer-1 else nout,
                                     # TODO: revise later
                                               bias=True if (i == nlayer-1 and not with_final_activation and bias)
                                               or (not with_norm) else False)  # set bias=False for BN
                                     for i in range(nlayer)])
        self.norms = nn.ModuleList([nn.BatchNorm1d(n_hid if i < nlayer-1 else nout) if with_norm else Identity()
                                    for i in range(nlayer)])
        self.nlayer = nlayer
        self.with_final_activation = with_final_activation
        self.residual = (nin == nout)  # TODO: test whether need this

    def reset_parameters(self):
        for layer, norm in zip(self.layers, self.norms):
            layer.reset_parameters()
            norm.reset_parameters()

    def forward(self, x):
        previous_x = x
        for i, (layer, norm) in enumerate(zip(self.layers, self.norms)):
            x = layer(x)
            if i < self.nlayer-1 or self.with_final_activation:
                x = norm(x)
                x = F.relu(x)

        # if self.residual:
        #     x = x + previous_x
        return x



if args.laplace_RW==True:
  from torch_geometric.transforms import AddLaplacianEigenvectorPE, AddRandomWalkPE
  tf=AddRandomWalkPE(walk_length=15, attr_name='rwe') 
else:
   tf=None


my_dataset='Peptides-struct'
dataset1 = LRGBDataset(root='./', name=my_dataset, transform=tf, split="train")#.shuffle()
validation_set1 = LRGBDataset(root='./', name=my_dataset,transform=tf, split="val")#.shuffle()
test_set1 = LRGBDataset(root='./', name=my_dataset,transform=tf, split="test")#.shuffle()

num_feats=dataset1.num_node_features
num_classes=dataset1.num_classes



#weights=torch.rand(args.comp,args.hidden,requires_grad=True).to(device)
# dataset1 = patchData(dataset1,args.comp,weights,device)
# validation_set1 = patchData(validation_set1,args.comp,weights,device)
# test_set1 = patchData(test_set1,args.comp,weights,device)


def pe(datasetz):
  outs=[]
  ins=[]
  dataset2=[]
  for p in range(len(datasetz)):
    try:
      tempo=datasetz[p]
      tempo['laplace']= torch.cat((datasetz[p].x,datasetz[p].rwe),dim=1)
      dataset2.append(tempo)
      ins.append(p)
    except:
      print(p)
      outs.append(p)
  return dataset2


if args.laplace_RW==True:
  dataset1 = pe(dataset1)
  validation_set1 = pe(validation_set1)
  test_set1 = pe(test_set1)


from torch_geometric.loader import DataLoader
trainloader = DataLoader(dataset1, batch_size=args.batch_size, shuffle=True)
valoader = DataLoader(validation_set1, batch_size=args.batch_size, shuffle=False)
testloader = DataLoader(test_set1, batch_size=args.batch_size, shuffle=False)


from torch import nn, einsum
import torch.nn.functional as F
from einops import rearrange, repeat

def exists(val):
    return val is not None

def default(val, d):
    return val if exists(val) else d


class CrossAttention(nn.Module):
    def __init__(self, query_dim, context_dim, heads = 3, dim_head = 64, dropout = 0.):
        super().__init__()
        inner_dim = dim_head * heads
        context_dim = default(context_dim, query_dim)

        self.scale = dim_head ** -0.5
        self.heads = heads

        self.to_q = nn.Linear(query_dim, inner_dim, bias = False)
        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)

        self.dropout = nn.Dropout(dropout)
        self.to_out = nn.Linear(inner_dim, query_dim)

    def forward(self, x, context , mask = None):
        h = self.heads

        q = self.to_q(x)
        context = default(context, x)
        k, v = self.to_kv(context).chunk(2, dim = -1)


        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))

        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale

        if exists(mask):
            mask = rearrange(mask, 'b ... -> b (...)')
            max_neg_value = -torch.finfo(sim.dtype).max
            mask = repeat(mask, 'b j -> (b h) () j', h = h)
            sim.masked_fill_(~mask, max_neg_value)

        # attention, what we cannot get enough of
        attn = sim.softmax(dim = -1)
        attn = self.dropout(attn)

        out = einsum('b i j, b j d -> b i d', attn, v)
        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)
        return self.to_out(out)


from torch_geometric.utils import to_undirected
from torch_geometric.data import Data
class HNO(torch.nn.Module):

    def __init__(self,):
        super(HNO, self).__init__()

        if args.laplace_RW==True:
          total_feats=num_feats+15
        else:
           total_feats=num_feats

        self.conv1 = GCNConv(total_feats, int(args.hidden))#,K=8)
        self.Inlin = Linear(total_feats, int(args.hidden))
        self.Inlin2 = Linear(int(args.hidden), int(args.hidden))
        # self.gin1 = GINConv(nn.Sequential(nn.Linear(num_feats, int(args.hidden)),
        #                                     #nn.ReLU(),
        #                                     #nn.Linear(args.hidden, args.hidden),
        #                                     nn.ReLU(),
        #                                     nn.Linear(int(args.hidden), int(args.hidden))))

        #self.conv1 = GCNConv(total_feats, int(2*args.hidden))
        self.conv2 = GCNConv(int(args.hidden), int(args.hidden))#K=8)
        self.conv3 = GCNConv(int(args.hidden), args.hidden)#,K=8)
        self.conv4 = GCNConv(int(args.hidden), args.hidden)#,K=8)
        
        self.convRev = GATConv( args.hidden, args.hidden)#,K=8)
        self.convRev2 = GCNConv(int(args.hidden), int(args.hidden))#,K=8)
        
        self.conv5 = GCNConv(int(args.hidden), args.hidden)#,K=8)
        # self.conv5 = ChebConv(int(args.hidden), args.hidden,K=8)
        #self.conv4 = SAGEConv(args.hidden, args.hidden)
        self.convGraph =  GCNConv(args.hidden,args.hidden)
        self.convGraph2 =  GCNConv(args.hidden,args.comp)

        self.Fconv = GCNConv(args.hidden*args.comp, num_classes)

        self.conv_sTc= GATConv(args.hidden, args.hidden)#GATConv(args.hidden, args.hidden)
        #self.Inlin = Linear(dataset.num_features, int(args.hidden))
        self.mlpComp=Linear(args.hidden, int(args.hidden))

        self.lin = Linear(args.hidden, int(args.hidden))
        self.lin2 = Linear(int(args.hidden), args.hidden)

        self.mlp= Linear(args.hidden*args.comp, num_classes)
        self.mlp2= Linear(args.hidden, num_classes)

        self.bano1 = torch.nn.BatchNorm1d(num_features= int(args.hidden))
        self.bano2 = torch.nn.BatchNorm1d(num_features= int(args.hidden))
        self.bano3 = torch.nn.BatchNorm1d(num_features= int(args.hidden))

        self.embLin=Linear(args.hidden, int(args.hidden))

        self.LaToGr= GATConv(int(args.hidden), int(args.hidden))

        #self.cross=CrossAttention(int(args.hidden),context_dim=int(args.hidden)) ## query dim, key_dim

        # if args.gConv=='GIN':
        #   self.conv_cTc = GINConv(nn.Sequential(nn.Linear(int(args.hidden), int(args.hidden)),
        #                                       #nn.ReLU(),
        #                                       #nn.Linear(args.hidden, args.hidden),
        #                                       nn.ReLU(),
        #                                       nn.Linear(int(args.hidden), int(args.hidden))))
        # elif args.gConv=='SAGE':
        #   self.conv_cTc= SAGEConv(args.hidden, args.hidden)

        if args.gConv=='GAT':
          self.conv_cTc= GATConv(args.hidden, args.hidden)

        # self.gin2 = GINConv(nn.Sequential(nn.Linear(args.hidden, args.hidden),
        #                                     nn.ReLU(),
        #                                     nn.Linear(args.hidden, args.hidden),
        #                                     nn.ReLU(),
        #                                     nn.Linear(args.hidden, args.hidden)))
        self.mlpRep = MLP(
            args.hidden, num_classes, nlayer=3, with_final_activation=False)


    def connect(self,adTop):
      temp=[]
      for k in range(args.comp):
        T = torch.cat([adTop[:k], adTop[k+1:]])
        temp+=T
      return torch.stack(temp)

    def extend(self,Gnodes,comp,device):
      adTop=torch.arange(0, Gnodes.shape[0]).to(device)
      adBot=torch.arange(Gnodes.shape[0], Gnodes.shape[0]+comp).to(device)

      top=adTop.repeat(comp)
      bot=adBot.repeat(Gnodes.shape[0],1).t().reshape(-1)
      indices = torch.stack((top, bot))
      return indices#top,bot


    def my_patches(self,N,M,device):
      rows=torch.arange(0,N)
      div=N%M ## comp
      ent=N-div
      g2=torch.arange(N,+N+M)
      g2=g2.repeat_interleave(int(ent/M), dim=0)
      if div !=0:
        g2=torch.cat((g2,g2[-1].repeat(div)))
      indices = torch.tensor([rows.tolist(), g2.tolist()], dtype=torch.long)
      return indices


    ###  insert the weights at the end of each graph in the batch
    def stack_rows(self,matrix, row_indices, weight):
        n, d = matrix.shape
        p = len(row_indices)
        new_n = n #+ k * p

        # Generate the stacked matrix
        stacked_matrix = torch.zeros(new_n, d, dtype=matrix.dtype, device=matrix.device)
        stacked_matrix[:n] = matrix

        # Stack the new matrix at the specified row indices
        k=weight.shape[0]
        for i, idx in enumerate(row_indices):
            stacked_matrix = torch.cat((stacked_matrix[:idx + i * k], weight, stacked_matrix[idx + i * k -k+ k:]))

        return stacked_matrix


    def transform_indices(self,A, B):

        n = len(A)
        m = max(B)

        shift = m - n+1

        # Step 2: Create mapping dictionary
        mapping = {}
        new_index = 0
        for value in set(B):
            mapping[value] = new_index
            new_index += 1

        # Step 3: Replace elements in B with new indices
        B_transformed = [mapping[value] for value in B]

        # Step 4: Create new tensor A
        A_transformed = list(range(shift, m + 1))

        return A_transformed, B_transformed

    def select_top_k_values(self, A, B, k):
        selected_values = []
        current_index = 0
        counter=0
        inds=[]
        for sample_size in B:
            # Select top-k values from the current sample
            sample_values = A[current_index : current_index + sample_size]
            selected_sample_values, indices = torch.topk(sample_values, k)

            # Append the selected values to the result
            selected_values.append(selected_sample_values)
            inds.append(indices+current_index-args.comp*counter)

            # Move to the next sample
            current_index += sample_size
            counter+=1

        # Concatenate the selected values from all samples
        result = torch.cat(selected_values)

        flatInd=torch.stack(inds).view(-1)
        # cater=torch.stack((ccc.repeat_interleave(3,dim=0),torch.stack(inds).repeat(1,3).view(-1)))
        return result, flatInd #orch.stack(inds).view(-1)

    def fcadj(self,bs,comp):
      nodes=torch.arange(0,comp*bs)
      top=nodes.repeat_interleave(comp, dim=0)
      bot=nodes.reshape(bs,comp).repeat_interleave(comp, dim=0).reshape(-1)
      return torch.stack((top,bot))

    def forward(self, x1, edge_index, batch, params,batch_size,device,data,pretrain):

        if args.use_graph==True:
          x = self.conv1(x1, edge_index)
          x = F.leaky_relu(x)
          x=self.bano1(x)
          #x = F.dropout(x, training=self.training,p=0.2)

          x = self.conv2(x, edge_index)
          x = F.leaky_relu(x)
          x=self.bano2(x)
          #x = F.dropout(x, training=self.training,p=0.2)

          x = self.conv3(x, edge_index)
          x = F.relu(x)
          x = self.bano3(x)
          # #x = F.dropout(x, training=self.training,p=0.2)

          x = self.conv4(x, edge_index)
          # x = F.relu(x)
          # x = self.bano3(x)
          # # #x = F.dropout(x, training=self.training,p=0.2)

          # x = self.conv5(x, edge_index)
          # x = F.relu(x)
          # x = self.bano3(x)
          # #x = F.dropout(x, training=self.training,p=0.2)

        params=self.embLin(params)
        params = params.relu()

        fuser=[]
        all_atts=[]
        lens=[] ## lengths of each unique graph
        latLens=[] ## lengths of each unique graph
        patch=[]


        c1=0
        c2=0
        co=0
        merged_samples=[]
        skipLens=[]
        sumi=0
        for i in range(batch_size):
          samples=x[batch==i]
          lens.append(samples.shape[0])
          latLens.append(torch.arange(c1+samples.shape[0],c1+samples.shape[0]+args.comp))
          skipLens.append(torch.arange(c2,samples.shape[0]+c2))
          c2+=samples.shape[0]+args.comp

          if args.patches==True:
             patch.append(self.my_patches(samples.shape[0],args.comp,device) +c1)
             sumi+=samples.shape[0]

          else:
              patch.append(self.extend(samples,args.comp,device) +c1)

          c1+=samples.shape[0]+args.comp
          co+=data[i].x.shape[0]
          merged_samples.append(samples)


        merged_samples=torch.cat(merged_samples)
        cumLens=torch.cumsum(torch.tensor(lens), dim=0).tolist()

        ## We stacked latent arrays after each graph G_i
        x2 = self.stack_rows(x, cumLens, params)

        patch=torch.cat(patch,dim=1).to(device)


        """
        Input to latent
        """

        newG=self.conv_sTc(x2,patch)#,return_attention_weights=True)
        latLens=torch.stack(latLens).reshape(-1)
        skipLens=torch.cat(skipLens,dim=0).reshape(-1)


        """
        Latent-to-Latent
        """
        latentNodes=newG[latLens].clone().to(device)
        temp=self.fcadj(len(lens),args.comp)  #torch_geometric.utils.dense_to_sparse(torch.ones(len(lens),args.comp,args.comp))[0]
        CompNodes=F.relu(self.conv_cTc(latentNodes,temp.cuda()))
        CompNodes=self.mlpComp(CompNodes)
        CompNodes=F.relu(CompNodes)
        CompNodes=CompNodes.reshape(len(lens),args.comp,args.hidden)
        CompNodes = torch.sum(CompNodes, dim=1)
        
        """
        Latent-to-Input
        """

        #x3 = self.stack_rows(x.clone(), cumLens, CompNodes)

        patch[[0,1]] = patch[[1,0]]

        final,atts=self.LaToGr(x2,patch.long(),return_attention_weights=True)

        mem=[]
        c0=0
        for el in lens:
          mem.append(torch.arange(c0,el+c0))
          c0+=el+args.comp
        mem=torch.cat(mem)

        indices = torch.where(atts[0][0,:] != atts[0][1,:])[0]
        a0=remove_self(atts[0]) ### Remove self-loops

        A=atts[1][indices,0]
        _,inds= self.select_top_k_values(A, (torch.tensor(lens)).tolist(), args.maxAtt)
        highest_weight_nodes=a0[1][inds]
        cater=torch.stack((highest_weight_nodes.repeat_interleave(args.maxAtt,dim=0),highest_weight_nodes.view(batch_size,args.maxAtt).repeat(1,args.maxAtt).view(-1)))
        cater=remove_self(cater)

        new_edges=torch.cat((data.edge_index,cater),dim=1)
        sorted_tensor, sorted_indices = torch.sort(new_edges[0])
        new_edges = new_edges[:, sorted_indices]

        x_up = self.convRev(x, new_edges)
        x_up = F.leaky_relu(x_up)
        # x_up = self.bano3(x_up)
        # #x = F.dropout(x, training=self.training,p=0.2)

        # x_up = self.convRev2(x_up, new_edges)
        #x = F.relu(x)
        #x = self.bano3(x)
        #x = F.dropout(x, training=self.training,p=0.2)
        

        final2 = global_add_pool(x, batch)
        final3 = global_add_pool(x_up, batch)
        final4=final2 +final3 + CompNodes #+ final1*0.8 #torch.cat((final2,CompNodes),dim=1)

        classifier=self.mlpRep(final4)
        #classifier=self.mlp(CompNodes)
        # classifier=F.leaky_relu(classifier)
        # classifier= F.dropout(classifier, training=self.training,p=0.3)
        # classifier=self.mlp2(classifier)
        cdd=0

        return classifier,atts, new_edges


def stack_rows(matrix, row_indices, k):
    n, d = matrix.shape
    p = len(row_indices)
    new_n = n #+ k * p

    # Generate the stacked matrix
    stacked_matrix = torch.zeros(new_n, d, dtype=matrix.dtype, device=matrix.device)
    stacked_matrix[:n] = matrix

    # Stack the new matrix at the specified row indices
    for i, idx in enumerate(row_indices):
        stacked_matrix = torch.cat((stacked_matrix[:idx + i * k], torch.ones(k, d), stacked_matrix[idx + i * k -k+ k:]))

    return stacked_matrix


if args.laplace_RW==True:
  eigs=args.k
else:
  eigs=0


from sklearn.metrics import average_precision_score
import numpy as np
def eval_ap(y_true, y_pred):
    '''
        compute Average Precision (AP) averaged across tasks
    '''
    ap_list = []
    y_true = y_true.cpu().detach().numpy()
    y_pred = y_pred.cpu().detach().numpy()

    for i in range(y_true.shape[1]):
        # AUC is only defined when there is at least one positive data.
        if np.sum(y_true[:, i] == 1) > 0 and np.sum(y_true[:, i] == 0) > 0:
            # ignore nan values
            is_labeled = y_true[:, i] == y_true[:, i]
            ap = average_precision_score(y_true[is_labeled, i],
                                         y_pred[is_labeled, i])

            ap_list.append(ap)

    if len(ap_list) == 0:
        raise RuntimeError(
            'No positively labeled data available. Cannot compute Average Precision.')

    return sum(ap_list) / len(ap_list)



#import matplotlib.pyplot as plt
lplot=[]
vplot=[]

model = HNO().to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)

criterion = torch.nn.L1Loss()

config = dict (
  Changes="Undirected",
  Latent_nodes=args.comp,
  hidden_dim=args.hidden,
  Laplacian=args.laplace,
  k_eigs=eigs,
  batch_size=args.batch_size,

  patches=args.patches,
  use_GNN=args.use_graph,
  learning_rate = args.lr,
  gConv = args.gConv,
  pretrain = args.pretrain,
  seed = args.seed
)


wandb.init(
project="ICML_AttRew_Struc",
name="GCN_backbone",
config=config,
)

def remove_self(att):
  a,b=att[0],att[1]

  # Find indices where a and b are not equal
  indices = torch.where(a != b)

  # Use the indices to get the desired elements
  result_a = a[indices]
  result_b = b[indices]
  return torch.stack((result_a,result_b))

import torch.optim as optim
from torch.optim import Adagrad, AdamW, Optimizer
import torch_geometric.graphgym.register as register
def get_cosine_schedule_with_warmup(
        optimizer: Optimizer, num_warmup_steps: int, num_training_steps: int,
        num_cycles: float = 0.5, last_epoch: int = -1):
    """
    Implementation by Huggingface:
    https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/optimization.py

    Create a schedule with a learning rate that decreases following the values
    of the cosine function between the initial lr set in the optimizer to 0,
    after a warmup period during which it increases linearly between 0 and the
    initial lr set in the optimizer.
    Args:
        optimizer ([`~torch.optim.Optimizer`]):
            The optimizer for which to schedule the learning rate.
        num_warmup_steps (`int`):
            The number of steps for the warmup phase.
        num_training_steps (`int`):
            The total number of training steps.
        num_cycles (`float`, *optional*, defaults to 0.5):
            The number of waves in the cosine schedule (the defaults is to just
            decrease from the max value to 0 following a half-cosine).
        last_epoch (`int`, *optional*, defaults to -1):
            The index of the last epoch when resuming training.
    Return:
        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.
    """

    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return max(1e-6, float(current_step) / float(max(1, num_warmup_steps)))
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))

    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)


@register.register_scheduler('cosine_with_warmup')
def cosine_with_warmup_scheduler(optimizer: Optimizer,
                                 num_warmup_epochs: int, max_epoch: int):
    scheduler = get_cosine_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=num_warmup_epochs,
        num_training_steps=max_epoch
    )
    return scheduler

scheduler=cosine_with_warmup_scheduler(optimizer,num_warmup_epochs=5,max_epoch=250)




temp=100000000
for epoch in range(250):
  model.train()
  correct = 0
  totalLoss=0
  total_loss = 0
  N = 0
  for i, data in enumerate(trainloader):

    data=data.to(device)

    optimizer.zero_grad()

    weights=torch.rand(args.comp,args.hidden,requires_grad=True).to(device)

    if args.laplace_RW==True:
      feats=data.laplace
    else:
      feats=data.x.float()

    #patches=data.patch_adj

    if args.pretrain==True:
      classify=model(feats,data.edge_index,data.batch,weights,data.batch.unique().shape[0],device,data,pretrain=args.pretrain)
    else:
      classify,atts, fuser=model(feats,data.edge_index,data.batch,weights,data.batch.unique().shape[0],device,data,pretrain=args.pretrain)

    mask = ~torch.isnan(data.y)

    loss = (classify[mask].squeeze() - data.y[mask]).abs().mean() #criterion(classify, data.y)  # Compute the loss

    loss.backward()

    total_loss += loss.item() * data.num_graphs
    N += data.num_graphs

    optimizer.step()

    totalLoss+=loss

  totalLoss=totalLoss / (i+1)
  
  train_loss = total_loss / N
  train_perf = train_loss

  scheduler.step()

  # if epoch %40==0:
  #     optimizer.param_groups[0]["lr"]=optimizer.param_groups[0]["lr"]*0.9

  val_correct=0
  #totalVaLoss=0
  total_val_loss=0
  Nval=0
  for j, valdata in enumerate(valoader):
    model.eval()
    valdata=valdata.to(device)

    if args.laplace_RW==True:
      valfeats=valdata.laplace
    else:
      valfeats=valdata.x.float()

    if args.pretrain==True:
      val_classify =model(valfeats,valdata.edge_index,valdata.batch,weights,valdata.batch.unique().shape[0],device,valdata,pretrain=args.pretrain)
    else:
      val_classify,val_atts,val_fuser=model(valfeats,valdata.edge_index,valdata.batch,weights,valdata.batch.unique().shape[0],device,valdata,pretrain=args.pretrain)
    
    valmask = ~torch.isnan(valdata.y)

    val_loss = criterion(val_classify, valdata.y)
    val_loss=(val_classify[valmask].squeeze() -valdata.y[valmask]).abs().mean()

    #totalVaLoss+=val_loss
    total_val_loss += val_loss.item()*valdata.num_graphs
    Nval += valdata.num_graphs

  Val_loss = total_val_loss/Nval
  val_perf = -Val_loss

  #totalVaLoss=totalVaLoss / (j+1)

  if val_loss<temp:
    temp=val_loss
    when=epoch
    torch.save({
        'epoch': epoch,
        'model': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_weights':weights,
          }, './'+args.webdata+'_'+str(epoch)+'.pth')

  #lplot.append(loss)
  #vplot.append(val_loss)

  print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f}, Val_Loss: {Val_loss:.4f}')

  wandb.log({"Train Loss": train_loss})
  wandb.log({"Val Loss": Val_loss})
  wandb.log({"train perf": train_perf})
  wandb.log({"Val perf": val_perf})
  wandb.log({"Epoch": epoch})



device="cuda"
checkpoint = torch.load('./'+args.webdata+'_'+str(when)+'.pth')
model.load_state_dict(checkpoint['model'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
for state in optimizer.state.values():
    for k, v in state.items():
        if isinstance(v, torch.Tensor):
            state[k] = v.to(device)

weights2=checkpoint['best_weights']
weights2=weights2.to(device)

totalTest=0

test_precision=0
total_test_loss=0
Ntest=0
with torch.no_grad():
  for k, testdata in enumerate(testloader):
    model.eval()
    model=model.to(device)
    testdata=testdata.to(device)
    testedgeAtt=testdata.edge_attr.float()

    if args.laplace_RW==True:
      testfeats=testdata.laplace
    else:
      testfeats=testdata.x.float()

    if args.pretrain==True:
      test_classify=model(testfeats,testdata.edge_index,testdata.batch,weights2,testdata.batch.unique().shape[0],device,pretrain=args.pretrain)
    else:
      #testfeats=add_lap(testdata)
       test_classify,val_atts,val_fuser=model(testfeats,testdata.edge_index,testdata.batch,weights2,testdata.batch.unique().shape[0],device,testdata,pretrain=args.pretrain)

    testmask = ~torch.isnan(testdata.y)

    testloss = (test_classify[testmask].squeeze() - testdata.y[testmask]).abs().mean()
    total_test_loss += testloss.item()*testdata.num_graphs
    Ntest += testdata.num_graphs

       #totalTest+=test_loss
test_loss = total_test_loss/Ntest
test_perf = -test_loss

#totalTest=totalTest / (k+1)

wandb.log({"Test Loss": test_loss})
wandb.log({"Test perf": test_perf})
wandb.log({"conv between latents": str(model.get_submodule('conv_cTc'))[:8]})
wandb.finish()