{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "# !pip install einops\n",
        "# !pip install wandb"
      ],
      "metadata": {
        "id": "v1m8XZNn-WrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "from itertools import product\n",
        "from typing import Callable, Optional\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, GINConv,global_mean_pool, ChebConv,global_add_pool\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import Tensor\n",
        "\n",
        "def index_to_mask(index: Tensor, size: Optional[int] = None) -> Tensor:\n",
        "    r\"\"\"Converts indices to a mask representation.\n",
        "\n",
        "    Args:\n",
        "        idx (Tensor): The indices.\n",
        "        size (int, optional). The size of the mask. If set to :obj:`None`, a\n",
        "            minimal sized output mask is returned.\n",
        "\n",
        "    Example:\n",
        "\n",
        "        >>> index = torch.tensor([1, 3, 5])\n",
        "        >>> index_to_mask(index)\n",
        "        tensor([False,  True, False,  True, False,  True])\n",
        "\n",
        "        >>> index_to_mask(index, size=7)\n",
        "        tensor([False,  True, False,  True, False,  True, False])\n",
        "    \"\"\"\n",
        "    index = index.view(-1)\n",
        "    size = int(index.max()) + 1 if size is None else size\n",
        "    mask = index.new_zeros(size, dtype=torch.bool)\n",
        "    mask[index] = True\n",
        "    return mask\n",
        "\n",
        "from torch_geometric.data import Data, HeteroData, InMemoryDataset\n",
        "from torch_geometric.utils import coalesce, remove_self_loops, to_undirected\n",
        "\n",
        "from torch_geometric.transforms import RandomNodeSplit\n",
        "from torch_geometric.graphgym.config import cfg\n",
        "\n",
        "# from torch_geometric.graphgym.utils.ben_utils import get_k_hop_adjacencies\n",
        "\n",
        "\n",
        "class RingTransferDataset(InMemoryDataset):\n",
        "    r\"\"\"A synthetic dataset that returns a Ring Transfer dataset.\n",
        "\n",
        "    Args:\n",
        "        num_graphs (int, optional): The number of graphs. (default: :obj:`1`)\n",
        "        num_nodes (int, optional): The average number of nodes in a graph.\n",
        "            (default: :obj:`1000`)\n",
        "        num_classes (int, optional): The number of node features.\n",
        "            (default: :obj:`64`)\n",
        "        task (str, optional): Whether to return node-level or graph-level\n",
        "            labels (:obj:`\"node\"`, :obj:`\"graph\"`, :obj:`\"auto\"`).\n",
        "            If set to :obj:`\"auto\"`, will return graph-level labels if\n",
        "            :obj:`num_graphs > 1`, and node-level labels other-wise.\n",
        "            (default: :obj:`\"auto\"`)\n",
        "        is_undirected (bool, optional): Whether the graphs to generate are\n",
        "            undirected. (default: :obj:`True`)\n",
        "        transform (callable, optional): A function/transform that takes in\n",
        "            an :obj:`torch_geometric.data.Data` object and returns a\n",
        "            transformed version. The data object will be transformed before\n",
        "            every access. (default: :obj:`None`)\n",
        "        **kwargs (optional): Additional attributes and their shapes\n",
        "            *e.g.* :obj:`global_features=5`.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_graphs,\n",
        "        num_nodes,\n",
        "        num_classes,\n",
        "        # task: str = \"auto\",\n",
        "        transform: Optional[Callable] = None,\n",
        "        pre_transform: Optional[Callable] = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__('.', transform)\n",
        "\n",
        "        self.num_graphs = num_graphs\n",
        "        self.num_nodes = num_nodes\n",
        "        self._num_classes = num_classes\n",
        "        self.kwargs = kwargs\n",
        "        # if cfg.gnn.layers_mp == 1: # the default - otherwise use specified no.\n",
        "\n",
        "        #     cfg.gnn.layers_mp = num_nodes//2\n",
        "        split = (self.num_graphs * torch.tensor([0.8, 0.1, 0.1])).long()\n",
        "        data_list, split = self.load_ring_transfer_dataset(self.num_nodes,\n",
        "                                                           split=split,\n",
        "                                                           classes=self._num_classes)\n",
        "\n",
        "        self.data, self.slices = self.collate(data_list)\n",
        "\n",
        "        # add train/val split masks\n",
        "        self.data.train_mask = index_to_mask(torch.tensor(split[0]), size=len(self.data.x))\n",
        "        self.data.val_mask = index_to_mask(torch.tensor(split[1]), size=len(self.data.x))\n",
        "        self.data.test_mask = index_to_mask(torch.tensor(split[2]), size=len(self.data.x))\n",
        "\n",
        "\n",
        "    def load_ring_transfer_dataset(self, nodes=10, split=[5000, 500, 500], classes=5):\n",
        "        train = self.generate_ring_transfer_graph_dataset(nodes, classes=classes, samples=split[0])\n",
        "        val = self.generate_ring_transfer_graph_dataset(nodes, classes=classes, samples=split[1])\n",
        "        test = self.generate_ring_transfer_graph_dataset(nodes, classes=classes, samples=split[2])\n",
        "        dataset = train + val + test\n",
        "        return dataset, [list(range(int(split[i]))) for i in range(3)]\n",
        "\n",
        "    def generate_ring_transfer_graph_dataset(self, nodes, classes=5, samples=10000):\n",
        "        # Generate the dataset\n",
        "        dataset = []\n",
        "        samples_per_class = torch.div(samples, classes, rounding_mode=\"floor\")\n",
        "        for i in range(samples):\n",
        "            label = torch.div(i, samples_per_class, rounding_mode=\"floor\")\n",
        "            target_class = np.zeros(classes)\n",
        "            target_class[label] = 1.0\n",
        "            graph = self.generate_ring_transfer_graph(nodes, target_class)\n",
        "            dataset.append(graph)\n",
        "        return dataset\n",
        "\n",
        "    def generate_ring_transfer_graph(self, nodes, target_label):\n",
        "        opposite_node = nodes // 2\n",
        "\n",
        "        # Initialise the feature matrix with a constant feature vector\n",
        "        x = np.ones((nodes, len(target_label)))\n",
        "\n",
        "        x[0, :] = 0.0\n",
        "        x[opposite_node, :] = target_label\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "\n",
        "        edge_index = []\n",
        "        for i in range(nodes-1):\n",
        "            edge_index.append([i, i + 1])\n",
        "            edge_index.append([i + 1, i])\n",
        "\n",
        "        # Add the edges that close the ring\n",
        "        edge_index.append([0, nodes - 1])\n",
        "        edge_index.append([nodes - 1, 0])\n",
        "        edge_index = torch.tensor(edge_index, dtype=torch.long).T\n",
        "\n",
        "        # Create a mask for the target node of the graph\n",
        "        mask = torch.zeros(nodes, dtype=torch.bool)\n",
        "        mask[0] = 1\n",
        "\n",
        "        # Add the label of the graph as a graph label\n",
        "        y = torch.tensor([np.argmax(target_label)], dtype=torch.long)\n",
        "\n",
        "        return Data(x=x, edge_index=edge_index, mask=mask, y=y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcAXCMHucaO8",
        "outputId": "9957769f-28a8-4f3c-ca1e-ca7b9b75aa59"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/graphgym/config.py:19: UserWarning: Could not define global config object. Please install 'yacs' via 'pip install yacs' in order to use GraphGym\n",
            "  warnings.warn(\"Could not define global config object. Please install \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/graphgym/imports.py:14: UserWarning: Please install 'pytorch_lightning' via  'pip install pytorch_lightning' in order to use GraphGym\n",
            "  warnings.warn(\"Please install 'pytorch_lightning' via  \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch_geometric.datasets import TUDataset, LRGBDataset\n",
        "import os.path as osp\n",
        "import torch_geometric.transforms as T\n",
        "import wandb\n",
        "from torch.utils.data import random_split\n",
        "from torch_geometric.loader import DataLoader\n",
        "import math\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, GINConv,global_mean_pool, ChebConv,global_add_pool\n",
        "#from torch_sparse import SparseTensor\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import random\n",
        "import torch_geometric\n",
        "from torch_geometric.utils import dropout_adj,dense_to_sparse\n",
        "from torch_geometric.utils import to_dense_adj,dense_to_sparse #dropout_adj,\n",
        "from torch_geometric.transforms import AddLaplacianEigenvectorPE, AddRandomWalkPE\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--comp', type=int, default=6, help='Latent Bottleneck')\n",
        "parser.add_argument('--hidden', type=int, default=64, help='Latent Dimension')\n",
        "parser.add_argument('--RW', type=int, default=8, help='Latent Dimension')\n",
        "parser.add_argument('--seed', type=int, default=12, help='Latent Dimension')\n",
        "parser.add_argument('--batch_size', type=int, default=32, help='Latent Dimension')\n",
        "parser.add_argument('--k', type=int, default=16, help='number of vectors')\n",
        "parser.add_argument('--laplace', type=bool, default=False, help='Use laplacian PE')\n",
        "parser.add_argument('--laplace_RW', type=bool, default=False, help='Use laplacian PE')\n",
        "parser.add_argument('--FA', type=bool, default=False, help='Use FA Layer')\n",
        "parser.add_argument('--learnable', type=bool, default=False, help='Use FA Layer')\n",
        "\n",
        "parser.add_argument('--use_graph', type=bool, default=True, help='Use graph infos')\n",
        "parser.add_argument('--patches', type=bool, default=True, help='Use graph infos')\n",
        "parser.add_argument('--pretrain', type=bool, default=False, help='Use graph infos')\n",
        "\n",
        "parser.add_argument('--use_weights', type=bool, default=False, help='Use graph infos')\n",
        "parser.add_argument('--gConv', type=str, default='GAT', help='graph conv between latents')\n",
        "parser.add_argument('--lr', type=float, default=0.0001, help='Learning rate')\n",
        "parser.add_argument('--lamb', type=float, default=0.8, help='Regularizer')\n",
        "parser.add_argument('--webdata', type=str, default='xwycycvu', help='which data')\n",
        "args = parser.parse_args([])"
      ],
      "metadata": {
        "id": "05wT1DGr9_GT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LapPE(edge_index, pos_enc_dim, num_nodes):\n",
        "    \"\"\"\n",
        "        Graph positional encoding v/ Laplacian eigenvectors\n",
        "    \"\"\"\n",
        "\n",
        "    # Laplacian\n",
        "    degree = torch_geometric.utils.degree(edge_index[0], num_nodes)\n",
        "    A = torch_geometric.utils.to_scipy_sparse_matrix(\n",
        "        edge_index, num_nodes=num_nodes)\n",
        "    N = sp.diags(np.array(degree.clip(1) ** -0.5, dtype=float))\n",
        "    L = sp.eye(num_nodes) - N * A * N\n",
        "\n",
        "    # Eigenvectors with numpy\n",
        "    EigVal, EigVec = np.linalg.eig(L.toarray())\n",
        "    idx = EigVal.argsort()  # increasing order\n",
        "    EigVal, EigVec = EigVal[idx], np.real(EigVec[:, idx])\n",
        "    PE = torch.from_numpy(EigVec[:, 1:pos_enc_dim+1]).float()\n",
        "    if PE.size(1) < pos_enc_dim:\n",
        "        zeros = torch.zeros(num_nodes, pos_enc_dim)\n",
        "        zeros[:, :PE.size(1)] = PE\n",
        "        PE = zeros\n",
        "    return PE\n",
        "\n",
        "\n",
        "def random_walk(A, n_iter):\n",
        "    # Geometric diffusion features with Random Walk\n",
        "    Dinv = A.sum(dim=-1).clamp(min=1).pow(-1).unsqueeze(-1)  # D^-1\n",
        "    RW = A * Dinv\n",
        "    M = RW\n",
        "    M_power = M\n",
        "    # Iterate\n",
        "    PE = [torch.diagonal(M)]\n",
        "    for _ in range(n_iter-1):\n",
        "        M_power = torch.matmul(M_power, M)\n",
        "        PE.append(torch.diagonal(M_power))\n",
        "    PE = torch.stack(PE, dim=-1)\n",
        "    return PE\n",
        "\n",
        "\n",
        "def RWSE(edge_index, pos_enc_dim, num_nodes):\n",
        "    \"\"\"\n",
        "        Initializing positional encoding with RWSE\n",
        "    \"\"\"\n",
        "    if edge_index.size(-1) == 0:\n",
        "        PE = torch.zeros(num_nodes, pos_enc_dim)\n",
        "    else:\n",
        "        A = torch_geometric.utils.to_dense_adj(\n",
        "            edge_index, max_num_nodes=num_nodes)[0]\n",
        "        PE = random_walk(A, pos_enc_dim)\n",
        "    return PE"
      ],
      "metadata": {
        "id": "WRPT3wM3S-A3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pe(datasetz):\n",
        "  outs=[]\n",
        "  ins=[]\n",
        "  dataset2=[]\n",
        "  for p in range(len(datasetz)):\n",
        "    try:\n",
        "      tempo=datasetz[p]\n",
        "      posi=RWSE(datasetz[p].edge_index, args.k, datasetz[p]['x'].shape[0])\n",
        "      tempo['laplace']=torch.cat((datasetz[p].x,posi),dim=1)\n",
        "      dataset2.append(tempo)\n",
        "      ins.append(p)\n",
        "    except:\n",
        "      print(p)\n",
        "      outs.append(p)\n",
        "  return dataset2\n",
        "\n",
        "\n",
        "dataset = RingTransferDataset(num_graphs=2000,num_nodes=60,num_classes=5)#.shuffle()\n",
        "dataset1=dataset[:int(2000*0.8)]\n",
        "validation_set1=dataset[int(2000*0.8):int(2000*0.9)]\n",
        "test_set1=dataset[int(2000*0.9):]\n",
        "# validation_set1 = RingTransferDataset(split=\"val\")#.shuffle()\n",
        "# test_set1 = RingTransferDataset(split=\"test\")#.shuffle()\n",
        "\n",
        "num_feats=dataset1.num_node_features\n",
        "num_classes=dataset1.num_classes\n",
        "\n",
        "if args.laplace==True:\n",
        "  dataset1 = pe(dataset1)\n",
        "  validation_set1 = pe(validation_set1)\n",
        "  test_set1 = pe(test_set1)"
      ],
      "metadata": {
        "id": "JHAm3OrN-CnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11eb9801-85f5-4910-fb37-8648f0cdc87f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:301: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim, heads = 3, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = default(context_dim, query_dim)\n",
        "\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
        "\n",
        "    def forward(self, x, context , mask = None):\n",
        "        h = self.heads\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        context = default(context, x)\n",
        "        k, v = self.to_kv(context).chunk(2, dim = -1)\n",
        "\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
        "\n",
        "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = rearrange(mask, 'b ... -> b (...)')\n",
        "            max_neg_value = -torch.finfo(sim.dtype).max\n",
        "            mask = repeat(mask, 'b j -> (b h) () j', h = h)\n",
        "            sim.masked_fill_(~mask, max_neg_value)\n",
        "\n",
        "        # attention, what we cannot get enough of\n",
        "        attn = sim.softmax(dim = -1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from torch_geometric.nn import global_add_pool\n",
        "\n",
        "BN = True\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "from torch_scatter import scatter\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, nin, nout, nlayer=2, with_final_activation=True, with_norm=True, bias=True):\n",
        "        super().__init__()\n",
        "        n_hid = nin\n",
        "        self.layers = nn.ModuleList([nn.Linear(nin if i == 0 else n_hid,\n",
        "                                     n_hid if i < nlayer-1 else nout,\n",
        "                                     # TODO: revise later\n",
        "                                               bias=True if (i == nlayer-1 and not with_final_activation and bias)\n",
        "                                               or (not with_norm) else False)  # set bias=False for BN\n",
        "                                     for i in range(nlayer)])\n",
        "        self.norms = nn.ModuleList([nn.BatchNorm1d(n_hid if i < nlayer-1 else nout) if with_norm else Identity()\n",
        "                                    for i in range(nlayer)])\n",
        "        self.nlayer = nlayer\n",
        "        self.with_final_activation = with_final_activation\n",
        "        self.residual = (nin == nout)  # TODO: test whether need this\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for layer, norm in zip(self.layers, self.norms):\n",
        "            layer.reset_parameters()\n",
        "            norm.reset_parameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        previous_x = x\n",
        "        for i, (layer, norm) in enumerate(zip(self.layers, self.norms)):\n",
        "            x = layer(x)\n",
        "            if i < self.nlayer-1 or self.with_final_activation:\n",
        "                x = norm(x)\n",
        "                x = F.relu(x)\n",
        "\n",
        "        # if self.residual:\n",
        "        #     x = x + previous_x\n",
        "        return x"
      ],
      "metadata": {
        "id": "f1VZ46bd-Eii"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch_geometric.utils import to_undirected\n",
        "from torch_geometric.data import Data\n",
        "class HNO(torch.nn.Module):\n",
        "\n",
        "    def __init__(self,):\n",
        "        super(HNO, self).__init__()\n",
        "\n",
        "        if args.laplace_RW==True:\n",
        "           total_feats=num_feats+args.RW\n",
        "\n",
        "        else:\n",
        "           total_feats=num_feats\n",
        "        self.conv1 = ChebConv(total_feats, int(2*args.hidden),K=8)\n",
        "        self.Inlin = Linear(total_feats, int(args.hidden))\n",
        "\n",
        "        self.Inlin2 = Linear(int(args.hidden), int(args.hidden))\n",
        "        # self.gin1 = GINConv(nn.Sequential(nn.Linear(num_feats, int(args.hidden)),\n",
        "        #                                     #nn.ReLU(),\n",
        "        #                                     #nn.Linear(args.hidden, args.hidden),\n",
        "        #                                     nn.ReLU(),\n",
        "        #                                     nn.Linear(int(args.hidden), int(args.hidden))))\n",
        "\n",
        "        #self.conv1 = GCNConv(num_feats, int(2*args.hidden))\n",
        "        self.conv2 = ChebConv(int(2*args.hidden), int(2*args.hidden),K=8)\n",
        "        self.conv3 = ChebConv(int(2*args.hidden), args.hidden,K=8)\n",
        "        self.conv4 = ChebConv(int(args.hidden), args.hidden,K=8)\n",
        "        # self.conv5 = ChebConv(int(args.hidden), args.hidden,K=8)\n",
        "        #self.conv4 = SAGEConv(args.hidden, args.hidden)\n",
        "        self.convGraph =  GCNConv(args.hidden,args.hidden)\n",
        "        self.convGraph2 =  GCNConv(args.hidden,args.comp)\n",
        "\n",
        "        self.Fconv = GCNConv(args.hidden*args.comp, num_classes)\n",
        "\n",
        "        self.conv_sTc= GATConv(args.hidden, args.hidden)#GATConv(args.hidden, args.hidden)\n",
        "        #self.Inlin = Linear(dataset.num_features, int(args.hidden))\n",
        "\n",
        "        self.lin = Linear(args.hidden, int(2*args.hidden))\n",
        "        self.lin2 = Linear(int(2*args.hidden), args.hidden)\n",
        "\n",
        "        self.mlp= Linear(args.hidden*args.comp, num_classes)\n",
        "        self.mlp2= Linear(args.hidden, num_classes)\n",
        "\n",
        "        self.bano1 = torch.nn.BatchNorm1d(num_features= int(2*args.hidden))\n",
        "        self.bano2 = torch.nn.BatchNorm1d(num_features= int(2*args.hidden))\n",
        "        self.bano3 = torch.nn.BatchNorm1d(num_features= int(args.hidden))\n",
        "\n",
        "        self.embLin=Linear(args.hidden, int(args.hidden))\n",
        "\n",
        "        self.LaToGr= GATConv(int(args.hidden), int(args.hidden))\n",
        "\n",
        "        #self.cross=CrossAttention(int(args.hidden),context_dim=int(args.hidden)) ## query dim, key_dim\n",
        "\n",
        "        # if args.gConv=='GIN':\n",
        "        #   self.conv_cTc = GINConv(nn.Sequential(nn.Linear(int(args.hidden), int(args.hidden)),\n",
        "        #                                       #nn.ReLU(),\n",
        "        #                                       #nn.Linear(args.hidden, args.hidden),\n",
        "        #                                       nn.ReLU(),\n",
        "        #                                       nn.Linear(int(args.hidden), int(args.hidden))))\n",
        "        # elif args.gConv=='SAGE':\n",
        "        #   self.conv_cTc= SAGEConv(args.hidden, args.hidden)\n",
        "\n",
        "        if args.gConv=='GAT':\n",
        "          self.conv_cTc= GATConv(args.hidden, args.hidden)\n",
        "\n",
        "        # self.gin2 = GINConv(nn.Sequential(nn.Linear(args.hidden, args.hidden),\n",
        "        #                                     nn.ReLU(),\n",
        "        #                                     nn.Linear(args.hidden, args.hidden),\n",
        "        #                                     nn.ReLU(),\n",
        "        #                                     nn.Linear(args.hidden, args.hidden)))\n",
        "        self.mlpRep = MLP(\n",
        "            int(args.hidden), num_classes, nlayer=2, with_final_activation=False)\n",
        "\n",
        "\n",
        "    def connect(self,adTop):\n",
        "      temp=[]\n",
        "      for k in range(args.comp):\n",
        "        T = torch.cat([adTop[:k], adTop[k+1:]])\n",
        "        temp+=T\n",
        "      return torch.stack(temp)\n",
        "\n",
        "    def extend(self,Gnodes,comp,device):\n",
        "      adTop=torch.arange(0, Gnodes.shape[0]).to(device)\n",
        "      adBot=torch.arange(Gnodes.shape[0], Gnodes.shape[0]+comp).to(device)\n",
        "\n",
        "      top=adTop.repeat(comp)\n",
        "      bot=adBot.repeat(Gnodes.shape[0],1).t().reshape(-1)\n",
        "      indices = torch.stack((top, bot))\n",
        "      return indices#top,bot\n",
        "\n",
        "    def adjacency_matrix(self,matrix):\n",
        "        # Get the shape of the input matrix\n",
        "        n= matrix.shape[0]\n",
        "\n",
        "        # Generate the fully connected adjacency matrix\n",
        "        row = torch.arange(n).repeat_interleave(n)\n",
        "        col = torch.arange(n).repeat(n)\n",
        "        edge_index = torch.stack([row, col], dim=0)\n",
        "        return edge_index\n",
        "\n",
        "    def my_patches(self,N,M,device):\n",
        "      rows=torch.arange(0,N)\n",
        "      div=N%M ## comp\n",
        "      ent=N-div\n",
        "      g2=torch.arange(N,+N+M)\n",
        "      g2=g2.repeat_interleave(int(ent/M), dim=0)\n",
        "      if div !=0:\n",
        "        g2=torch.cat((g2,g2[-1].repeat(div)))\n",
        "      indices = torch.tensor([rows.tolist(), g2.tolist()], dtype=torch.long)\n",
        "      return indices\n",
        "\n",
        "\n",
        "    def patches(self,N, M,device):\n",
        "\n",
        "        # Calculate the number of connections per group\n",
        "        connections_per_group = N // M\n",
        "\n",
        "        # Generate the indices of connected nodes\n",
        "        row_indices = []\n",
        "        col_indices = []\n",
        "\n",
        "        for i in range(M):\n",
        "            start_index = i * connections_per_group\n",
        "            end_index = start_index + connections_per_group\n",
        "\n",
        "            row_indices.extend(range(start_index, end_index))\n",
        "            col_indices.extend([N + i] * connections_per_group)\n",
        "\n",
        "        if (N+M)%2 !=0:\n",
        "                row_indices.append(N-1)\n",
        "                perm = torch.randperm(len(col_indices))\n",
        "                idx = perm[0]\n",
        "                indi = col_indices[idx]\n",
        "\n",
        "                col_indices.append(indi)\n",
        "\n",
        "\n",
        "        # Create the sparse tensor\n",
        "        #random.shuffle(col_indices)            ### When commented, removes random shuffling of columns in the end\n",
        "        indices = torch.tensor([row_indices, col_indices], dtype=torch.long)\n",
        "\n",
        "        return indices\n",
        "\n",
        "    ###  insert the weights at the end of each graph in the batch\n",
        "    def stack_rows(self,matrix, row_indices, weight):\n",
        "        n, d = matrix.shape\n",
        "        p = len(row_indices)\n",
        "        new_n = n #+ k * p\n",
        "\n",
        "        # Generate the stacked matrix\n",
        "        stacked_matrix = torch.zeros(new_n, d, dtype=matrix.dtype, device=matrix.device)\n",
        "        stacked_matrix[:n] = matrix\n",
        "\n",
        "        # Stack the new matrix at the specified row indices\n",
        "        k=weight.shape[0]\n",
        "        for i, idx in enumerate(row_indices):\n",
        "            stacked_matrix = torch.cat((stacked_matrix[:idx + i * k], weight, stacked_matrix[idx + i * k -k+ k:]))\n",
        "\n",
        "        return stacked_matrix\n",
        "\n",
        "\n",
        "    def transform_indices(self,A, B):\n",
        "\n",
        "        n = len(A)\n",
        "        m = max(B)\n",
        "\n",
        "        shift = m - n+1\n",
        "\n",
        "        # Step 2: Create mapping dictionary\n",
        "        mapping = {}\n",
        "        new_index = 0\n",
        "        for value in set(B):\n",
        "            mapping[value] = new_index\n",
        "            new_index += 1\n",
        "\n",
        "        # Step 3: Replace elements in B with new indices\n",
        "        B_transformed = [mapping[value] for value in B]\n",
        "\n",
        "        # Step 4: Create new tensor A\n",
        "        A_transformed = list(range(shift, m + 1))\n",
        "\n",
        "        return A_transformed, B_transformed\n",
        "\n",
        "    def forward(self, x, edge_index, batch, params,batch_size,device,data,pretrain):\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x=self.bano1(x)\n",
        "        x = F.dropout(x, training=self.training,p=0.2)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x=self.bano2(x)\n",
        "        x = F.dropout(x, training=self.training,p=0.2)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.bano3(x)\n",
        "        x = F.dropout(x, training=self.training,p=0.2)\n",
        "\n",
        "        x = self.conv4(x, edge_index)\n",
        "\n",
        "        classifier=self.mlpRep(x)\n",
        "        #classifier=self.mlp(CompNodes)\n",
        "        # classifier=F.leaky_relu(classifier)\n",
        "        # classifier= F.dropout(classifier, training=self.training,p=0.3)\n",
        "        # classifier=self.mlp2(classifier)\n",
        "        cdd=0\n",
        "\n",
        "        return classifier,cdd, cdd"
      ],
      "metadata": {
        "id": "n5jvEQbP-Hhj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "if args.laplace==True:\n",
        "  eigs=args.k\n",
        "else:\n",
        "  eigs=0\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "lplot=[]\n",
        "vplot=[]\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "model = HNO().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "trainloader = DataLoader(dataset1, batch_size=args.batch_size, shuffle=True,drop_last=True)\n",
        "valoader = DataLoader(validation_set1, batch_size=16, shuffle=False,drop_last=True)\n",
        "testloader = DataLoader(test_set1, batch_size=16, shuffle=False,drop_last=True)"
      ],
      "metadata": {
        "id": "aofWGO8R-MLz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvItbvJi96LX",
        "outputId": "0ca15f7e-e46c-452c-f938-2942d0e64cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Loss: 1.7126,Train Acc: 0.1956, Val_Loss: 1.8426,Val Acc: 0.2083\n",
            "Epoch: 001, Loss: 1.6762,Train Acc: 0.1787, Val_Loss: 1.9571,Val Acc: 0.2083\n",
            "Epoch: 002, Loss: 1.6164,Train Acc: 0.1856, Val_Loss: 1.5511,Val Acc: 0.1667\n",
            "Epoch: 003, Loss: 1.6402,Train Acc: 0.1881, Val_Loss: 1.6271,Val Acc: 0.2083\n",
            "Epoch: 004, Loss: 1.6058,Train Acc: 0.1944, Val_Loss: 1.6080,Val Acc: 0.2083\n",
            "Epoch: 005, Loss: 1.6287,Train Acc: 0.1919, Val_Loss: 1.5677,Val Acc: 0.2083\n",
            "Epoch: 006, Loss: 1.5556,Train Acc: 0.2137, Val_Loss: 1.7786,Val Acc: 0.2083\n",
            "Epoch: 007, Loss: 1.5973,Train Acc: 0.2144, Val_Loss: 1.6354,Val Acc: 0.2083\n",
            "Epoch: 008, Loss: 1.6168,Train Acc: 0.2194, Val_Loss: 1.6891,Val Acc: 0.2083\n",
            "Epoch: 009, Loss: 1.6616,Train Acc: 0.1963, Val_Loss: 1.7497,Val Acc: 0.2083\n",
            "Epoch: 010, Loss: 1.5861,Train Acc: 0.1994, Val_Loss: 1.6466,Val Acc: 0.2083\n",
            "Epoch: 011, Loss: 1.6497,Train Acc: 0.2056, Val_Loss: 1.6641,Val Acc: 0.2083\n",
            "Epoch: 012, Loss: 1.6337,Train Acc: 0.2013, Val_Loss: 1.5685,Val Acc: 0.2083\n",
            "Epoch: 013, Loss: 1.5946,Train Acc: 0.1925, Val_Loss: 1.7035,Val Acc: 0.2083\n",
            "Epoch: 014, Loss: 1.6191,Train Acc: 0.2031, Val_Loss: 1.7188,Val Acc: 0.2083\n",
            "Epoch: 015, Loss: 1.5832,Train Acc: 0.2131, Val_Loss: 1.7214,Val Acc: 0.2083\n",
            "Epoch: 016, Loss: 1.5849,Train Acc: 0.2075, Val_Loss: 1.6832,Val Acc: 0.2083\n",
            "Epoch: 017, Loss: 1.6049,Train Acc: 0.1956, Val_Loss: 1.6640,Val Acc: 0.2083\n",
            "Epoch: 018, Loss: 1.6012,Train Acc: 0.2050, Val_Loss: 1.6763,Val Acc: 0.2083\n",
            "Epoch: 019, Loss: 1.6278,Train Acc: 0.2069, Val_Loss: 1.5968,Val Acc: 0.2083\n",
            "Epoch: 020, Loss: 1.6031,Train Acc: 0.2081, Val_Loss: 1.6474,Val Acc: 0.2083\n",
            "Epoch: 021, Loss: 1.6011,Train Acc: 0.2056, Val_Loss: 1.5620,Val Acc: 0.1667\n",
            "Epoch: 022, Loss: 1.5844,Train Acc: 0.2087, Val_Loss: 1.6859,Val Acc: 0.2083\n",
            "Epoch: 023, Loss: 1.6291,Train Acc: 0.1988, Val_Loss: 1.6241,Val Acc: 0.2083\n",
            "Epoch: 024, Loss: 1.6021,Train Acc: 0.2137, Val_Loss: 1.6383,Val Acc: 0.2083\n",
            "Epoch: 025, Loss: 1.6258,Train Acc: 0.1888, Val_Loss: 1.6619,Val Acc: 0.2083\n",
            "Epoch: 026, Loss: 1.5790,Train Acc: 0.2006, Val_Loss: 1.6727,Val Acc: 0.2083\n",
            "Epoch: 027, Loss: 1.6120,Train Acc: 0.2150, Val_Loss: 1.6790,Val Acc: 0.2083\n",
            "Epoch: 028, Loss: 1.6213,Train Acc: 0.2106, Val_Loss: 1.6087,Val Acc: 0.2083\n",
            "Epoch: 029, Loss: 1.6171,Train Acc: 0.2156, Val_Loss: 1.6327,Val Acc: 0.2083\n",
            "Epoch: 030, Loss: 1.6301,Train Acc: 0.1969, Val_Loss: 1.5977,Val Acc: 0.2083\n",
            "Epoch: 031, Loss: 1.6144,Train Acc: 0.2013, Val_Loss: 1.5986,Val Acc: 0.2083\n",
            "Epoch: 032, Loss: 1.5972,Train Acc: 0.2137, Val_Loss: 1.6086,Val Acc: 0.2083\n",
            "Epoch: 033, Loss: 1.6192,Train Acc: 0.2025, Val_Loss: 1.6020,Val Acc: 0.2083\n",
            "Epoch: 034, Loss: 1.6197,Train Acc: 0.2006, Val_Loss: 1.6146,Val Acc: 0.2083\n",
            "Epoch: 035, Loss: 1.5850,Train Acc: 0.2100, Val_Loss: 1.6524,Val Acc: 0.2083\n",
            "Epoch: 036, Loss: 1.6346,Train Acc: 0.2094, Val_Loss: 1.6245,Val Acc: 0.2083\n",
            "Epoch: 037, Loss: 1.6337,Train Acc: 0.2162, Val_Loss: 1.6804,Val Acc: 0.2083\n",
            "Epoch: 038, Loss: 1.6025,Train Acc: 0.1975, Val_Loss: 1.6302,Val Acc: 0.2083\n",
            "Epoch: 039, Loss: 1.6024,Train Acc: 0.2087, Val_Loss: 1.6246,Val Acc: 0.2083\n",
            "Epoch: 040, Loss: 1.6082,Train Acc: 0.1913, Val_Loss: 1.5941,Val Acc: 0.2083\n",
            "Epoch: 041, Loss: 1.5879,Train Acc: 0.2162, Val_Loss: 1.5879,Val Acc: 0.2083\n",
            "Epoch: 042, Loss: 1.6181,Train Acc: 0.2162, Val_Loss: 1.5680,Val Acc: 0.1667\n",
            "Epoch: 043, Loss: 1.6011,Train Acc: 0.2094, Val_Loss: 1.6019,Val Acc: 0.2083\n",
            "Epoch: 044, Loss: 1.5968,Train Acc: 0.2194, Val_Loss: 1.5782,Val Acc: 0.2083\n",
            "Epoch: 045, Loss: 1.6180,Train Acc: 0.2206, Val_Loss: 1.6474,Val Acc: 0.2083\n",
            "Epoch: 046, Loss: 1.5864,Train Acc: 0.2200, Val_Loss: 1.5464,Val Acc: 0.1667\n",
            "Epoch: 047, Loss: 1.5863,Train Acc: 0.2206, Val_Loss: 1.5727,Val Acc: 0.1667\n",
            "Epoch: 048, Loss: 1.6134,Train Acc: 0.2175, Val_Loss: 1.6409,Val Acc: 0.2083\n",
            "Epoch: 049, Loss: 1.5833,Train Acc: 0.2319, Val_Loss: 1.5845,Val Acc: 0.1667\n",
            "Epoch: 050, Loss: 1.6438,Train Acc: 0.2106, Val_Loss: 1.5651,Val Acc: 0.1667\n",
            "Epoch: 051, Loss: 1.6115,Train Acc: 0.2081, Val_Loss: 1.5818,Val Acc: 0.2083\n",
            "Epoch: 052, Loss: 1.6250,Train Acc: 0.2256, Val_Loss: 1.5484,Val Acc: 0.1667\n",
            "Epoch: 053, Loss: 1.6241,Train Acc: 0.2025, Val_Loss: 1.6159,Val Acc: 0.2083\n",
            "Epoch: 054, Loss: 1.5979,Train Acc: 0.2269, Val_Loss: 1.6043,Val Acc: 0.2083\n",
            "Epoch: 055, Loss: 1.5988,Train Acc: 0.2194, Val_Loss: 1.5859,Val Acc: 0.2083\n",
            "Epoch: 056, Loss: 1.5836,Train Acc: 0.2281, Val_Loss: 1.6141,Val Acc: 0.2083\n",
            "Epoch: 057, Loss: 1.6226,Train Acc: 0.2206, Val_Loss: 1.6135,Val Acc: 0.2083\n",
            "Epoch: 058, Loss: 1.6018,Train Acc: 0.2281, Val_Loss: 1.6240,Val Acc: 0.2083\n",
            "Epoch: 059, Loss: 1.6018,Train Acc: 0.2050, Val_Loss: 1.5654,Val Acc: 0.1667\n",
            "Epoch: 060, Loss: 1.6001,Train Acc: 0.2313, Val_Loss: 1.6430,Val Acc: 0.2083\n",
            "Epoch: 061, Loss: 1.5986,Train Acc: 0.2256, Val_Loss: 1.5843,Val Acc: 0.1667\n",
            "Epoch: 062, Loss: 1.6133,Train Acc: 0.2206, Val_Loss: 1.6148,Val Acc: 0.2083\n",
            "Epoch: 063, Loss: 1.5969,Train Acc: 0.2213, Val_Loss: 1.6104,Val Acc: 0.2083\n",
            "Epoch: 064, Loss: 1.6289,Train Acc: 0.2256, Val_Loss: 1.5839,Val Acc: 0.1667\n",
            "Epoch: 065, Loss: 1.6103,Train Acc: 0.2013, Val_Loss: 1.6082,Val Acc: 0.2083\n",
            "Epoch: 066, Loss: 1.5920,Train Acc: 0.2188, Val_Loss: 1.6340,Val Acc: 0.2083\n",
            "Epoch: 067, Loss: 1.6123,Train Acc: 0.2281, Val_Loss: 1.6574,Val Acc: 0.2083\n",
            "Epoch: 068, Loss: 1.5811,Train Acc: 0.2213, Val_Loss: 1.5910,Val Acc: 0.1667\n",
            "Epoch: 069, Loss: 1.6103,Train Acc: 0.2244, Val_Loss: 1.6131,Val Acc: 0.2083\n",
            "Epoch: 070, Loss: 1.5678,Train Acc: 0.2175, Val_Loss: 1.5970,Val Acc: 0.2083\n",
            "Epoch: 071, Loss: 1.6350,Train Acc: 0.2156, Val_Loss: 1.5936,Val Acc: 0.1667\n",
            "Epoch: 072, Loss: 1.6057,Train Acc: 0.2188, Val_Loss: 1.6014,Val Acc: 0.2083\n",
            "Epoch: 073, Loss: 1.5807,Train Acc: 0.2181, Val_Loss: 1.5795,Val Acc: 0.1667\n",
            "Epoch: 074, Loss: 1.6071,Train Acc: 0.2419, Val_Loss: 1.5772,Val Acc: 0.1667\n",
            "Epoch: 075, Loss: 1.5862,Train Acc: 0.2344, Val_Loss: 1.6335,Val Acc: 0.2083\n",
            "Epoch: 076, Loss: 1.6277,Train Acc: 0.2194, Val_Loss: 1.5371,Val Acc: 0.1667\n",
            "Epoch: 077, Loss: 1.5931,Train Acc: 0.2275, Val_Loss: 1.5571,Val Acc: 0.1667\n",
            "Epoch: 078, Loss: 1.5605,Train Acc: 0.2319, Val_Loss: 1.5883,Val Acc: 0.2083\n",
            "Epoch: 079, Loss: 1.5831,Train Acc: 0.2275, Val_Loss: 1.6043,Val Acc: 0.2083\n",
            "Epoch: 080, Loss: 1.6453,Train Acc: 0.2375, Val_Loss: 1.5851,Val Acc: 0.1667\n",
            "Epoch: 081, Loss: 1.6264,Train Acc: 0.2150, Val_Loss: 1.5804,Val Acc: 0.1667\n",
            "Epoch: 082, Loss: 1.6157,Train Acc: 0.2269, Val_Loss: 1.6362,Val Acc: 0.2083\n",
            "Epoch: 083, Loss: 1.6138,Train Acc: 0.2194, Val_Loss: 1.5651,Val Acc: 0.2083\n",
            "Epoch: 084, Loss: 1.6017,Train Acc: 0.2362, Val_Loss: 1.5876,Val Acc: 0.2083\n",
            "Epoch: 085, Loss: 1.5941,Train Acc: 0.2206, Val_Loss: 1.5630,Val Acc: 0.1667\n",
            "Epoch: 086, Loss: 1.5859,Train Acc: 0.2150, Val_Loss: 1.6043,Val Acc: 0.2083\n",
            "Epoch: 087, Loss: 1.6031,Train Acc: 0.2269, Val_Loss: 1.5993,Val Acc: 0.2083\n",
            "Epoch: 088, Loss: 1.6211,Train Acc: 0.2169, Val_Loss: 1.5810,Val Acc: 0.2083\n",
            "Epoch: 089, Loss: 1.6140,Train Acc: 0.2550, Val_Loss: 1.5621,Val Acc: 0.1667\n",
            "Epoch: 090, Loss: 1.5967,Train Acc: 0.2344, Val_Loss: 1.5361,Val Acc: 0.1667\n",
            "Epoch: 091, Loss: 1.5934,Train Acc: 0.2487, Val_Loss: 1.5694,Val Acc: 0.1667\n",
            "Epoch: 092, Loss: 1.6193,Train Acc: 0.2362, Val_Loss: 1.5367,Val Acc: 0.1667\n",
            "Epoch: 093, Loss: 1.6014,Train Acc: 0.2512, Val_Loss: 1.6055,Val Acc: 0.2083\n",
            "Epoch: 094, Loss: 1.6319,Train Acc: 0.2125, Val_Loss: 1.5764,Val Acc: 0.1667\n",
            "Epoch: 095, Loss: 1.5976,Train Acc: 0.2381, Val_Loss: 1.5685,Val Acc: 0.1667\n",
            "Epoch: 096, Loss: 1.5984,Train Acc: 0.2162, Val_Loss: 1.5878,Val Acc: 0.2083\n",
            "Epoch: 097, Loss: 1.6285,Train Acc: 0.2456, Val_Loss: 1.5655,Val Acc: 0.1667\n",
            "Epoch: 098, Loss: 1.6027,Train Acc: 0.2225, Val_Loss: 1.6100,Val Acc: 0.2083\n",
            "Epoch: 099, Loss: 1.6194,Train Acc: 0.2281, Val_Loss: 1.5896,Val Acc: 0.2083\n",
            "Epoch: 100, Loss: 1.5981,Train Acc: 0.2288, Val_Loss: 1.6621,Val Acc: 0.2083\n",
            "Epoch: 101, Loss: 1.5854,Train Acc: 0.2394, Val_Loss: 1.5224,Val Acc: 0.1667\n",
            "Epoch: 102, Loss: 1.6181,Train Acc: 0.2325, Val_Loss: 1.5228,Val Acc: 0.1667\n",
            "Epoch: 103, Loss: 1.5995,Train Acc: 0.2313, Val_Loss: 1.6264,Val Acc: 0.2083\n",
            "Epoch: 104, Loss: 1.6134,Train Acc: 0.2475, Val_Loss: 1.5832,Val Acc: 0.1667\n",
            "Epoch: 105, Loss: 1.5660,Train Acc: 0.2469, Val_Loss: 1.5659,Val Acc: 0.1667\n",
            "Epoch: 106, Loss: 1.6059,Train Acc: 0.2450, Val_Loss: 1.5504,Val Acc: 0.1667\n",
            "Epoch: 107, Loss: 1.6337,Train Acc: 0.2319, Val_Loss: 1.5819,Val Acc: 0.1667\n",
            "Epoch: 108, Loss: 1.5601,Train Acc: 0.2487, Val_Loss: 1.5982,Val Acc: 0.2083\n",
            "Epoch: 109, Loss: 1.6119,Train Acc: 0.2200, Val_Loss: 1.5715,Val Acc: 0.1667\n",
            "Epoch: 110, Loss: 1.6215,Train Acc: 0.2219, Val_Loss: 1.6003,Val Acc: 0.2083\n",
            "Epoch: 111, Loss: 1.6041,Train Acc: 0.2462, Val_Loss: 1.5921,Val Acc: 0.1667\n",
            "Epoch: 112, Loss: 1.5895,Train Acc: 0.2569, Val_Loss: 1.5905,Val Acc: 0.2083\n",
            "Epoch: 113, Loss: 1.6108,Train Acc: 0.2481, Val_Loss: 1.6390,Val Acc: 0.2083\n",
            "Epoch: 114, Loss: 1.5642,Train Acc: 0.2362, Val_Loss: 1.5659,Val Acc: 0.1667\n",
            "Epoch: 115, Loss: 1.6052,Train Acc: 0.2350, Val_Loss: 1.5935,Val Acc: 0.1667\n",
            "Epoch: 116, Loss: 1.5828,Train Acc: 0.2450, Val_Loss: 1.6078,Val Acc: 0.2083\n",
            "Epoch: 117, Loss: 1.5915,Train Acc: 0.2375, Val_Loss: 1.5768,Val Acc: 0.2083\n",
            "Epoch: 118, Loss: 1.6042,Train Acc: 0.2531, Val_Loss: 1.5931,Val Acc: 0.1667\n",
            "Epoch: 119, Loss: 1.6118,Train Acc: 0.2412, Val_Loss: 1.5239,Val Acc: 0.1667\n",
            "Epoch: 120, Loss: 1.6045,Train Acc: 0.2525, Val_Loss: 1.6182,Val Acc: 0.2083\n",
            "Epoch: 121, Loss: 1.6092,Train Acc: 0.2238, Val_Loss: 1.5762,Val Acc: 0.2083\n",
            "Epoch: 122, Loss: 1.6115,Train Acc: 0.2512, Val_Loss: 1.5952,Val Acc: 0.2083\n",
            "Epoch: 123, Loss: 1.6036,Train Acc: 0.2619, Val_Loss: 1.6209,Val Acc: 0.2083\n",
            "Epoch: 124, Loss: 1.6062,Train Acc: 0.2300, Val_Loss: 1.5851,Val Acc: 0.2083\n",
            "Epoch: 125, Loss: 1.5993,Train Acc: 0.2462, Val_Loss: 1.6766,Val Acc: 0.2083\n",
            "Epoch: 126, Loss: 1.6121,Train Acc: 0.2531, Val_Loss: 1.6544,Val Acc: 0.2083\n",
            "Epoch: 127, Loss: 1.6021,Train Acc: 0.2400, Val_Loss: 1.6075,Val Acc: 0.2083\n",
            "Epoch: 128, Loss: 1.5866,Train Acc: 0.2500, Val_Loss: 1.5518,Val Acc: 0.1667\n",
            "Epoch: 129, Loss: 1.5754,Train Acc: 0.2650, Val_Loss: 1.6020,Val Acc: 0.2083\n",
            "Epoch: 130, Loss: 1.5963,Train Acc: 0.2531, Val_Loss: 1.6181,Val Acc: 0.2083\n",
            "Epoch: 131, Loss: 1.6088,Train Acc: 0.2412, Val_Loss: 1.5368,Val Acc: 0.1667\n",
            "Epoch: 132, Loss: 1.5904,Train Acc: 0.2487, Val_Loss: 1.5754,Val Acc: 0.1667\n",
            "Epoch: 133, Loss: 1.5707,Train Acc: 0.2494, Val_Loss: 1.6306,Val Acc: 0.2083\n",
            "Epoch: 134, Loss: 1.5957,Train Acc: 0.2512, Val_Loss: 1.6180,Val Acc: 0.2083\n",
            "Epoch: 135, Loss: 1.5840,Train Acc: 0.2656, Val_Loss: 1.5924,Val Acc: 0.1667\n",
            "Epoch: 136, Loss: 1.6127,Train Acc: 0.2500, Val_Loss: 1.6229,Val Acc: 0.2083\n",
            "Epoch: 137, Loss: 1.5624,Train Acc: 0.2400, Val_Loss: 1.5364,Val Acc: 0.1667\n",
            "Epoch: 138, Loss: 1.6150,Train Acc: 0.2281, Val_Loss: 1.5621,Val Acc: 0.1667\n",
            "Epoch: 139, Loss: 1.6296,Train Acc: 0.2456, Val_Loss: 1.5555,Val Acc: 0.1667\n",
            "Epoch: 140, Loss: 1.6231,Train Acc: 0.2313, Val_Loss: 1.5405,Val Acc: 0.1667\n",
            "Epoch: 141, Loss: 1.5922,Train Acc: 0.2512, Val_Loss: 1.5948,Val Acc: 0.1667\n",
            "Epoch: 142, Loss: 1.6173,Train Acc: 0.2612, Val_Loss: 1.6328,Val Acc: 0.2083\n",
            "Epoch: 143, Loss: 1.6110,Train Acc: 0.2500, Val_Loss: 1.6286,Val Acc: 0.2083\n",
            "Epoch: 144, Loss: 1.5948,Train Acc: 0.2381, Val_Loss: 1.5239,Val Acc: 0.1667\n",
            "Epoch: 145, Loss: 1.5830,Train Acc: 0.2450, Val_Loss: 1.6027,Val Acc: 0.2083\n",
            "Epoch: 146, Loss: 1.5401,Train Acc: 0.2306, Val_Loss: 1.6189,Val Acc: 0.2083\n",
            "Epoch: 147, Loss: 1.6136,Train Acc: 0.2375, Val_Loss: 1.6081,Val Acc: 0.2083\n",
            "Epoch: 148, Loss: 1.5840,Train Acc: 0.2637, Val_Loss: 1.6222,Val Acc: 0.2083\n",
            "Epoch: 149, Loss: 1.5902,Train Acc: 0.2462, Val_Loss: 1.6146,Val Acc: 0.2083\n"
          ]
        }
      ],
      "source": [
        "\n",
        "temp=0\n",
        "for epoch in range(150):\n",
        "  model.train()\n",
        "  correct = 0\n",
        "\n",
        "  totalLoss=0\n",
        "  totalAcc=0\n",
        "  for i, data in enumerate(trainloader):\n",
        "\n",
        "    data=data.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    weights=torch.rand(args.comp,args.hidden,requires_grad=True).to(device)\n",
        "\n",
        "    if args.laplace==True:\n",
        "      feats=data.laplace\n",
        "    else:\n",
        "      feats=data.x.float()\n",
        "\n",
        "    classify,atts, fuser=model(feats,data.edge_index,data.batch,weights,data.batch.unique().shape[0],device,data,pretrain=args.pretrain)\n",
        "\n",
        "    loss = criterion(classify[data.mask], data.y)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    pred = classify.argmax(dim=1)  # Use the class with highest probability.\n",
        "    correct = pred[data.mask] == data.y # Check against ground-truth labels.\n",
        "    train_acc = int(correct.sum()) / int(data.mask.sum())  # Derive ratio of correct predictions.\n",
        "\n",
        "    totalAcc+=train_acc\n",
        "\n",
        "  totalAcc/=(i+1)\n",
        "\n",
        "  if epoch %10==0:\n",
        "      optimizer.param_groups[0][\"lr\"]=optimizer.param_groups[0][\"lr\"]*0.95\n",
        "\n",
        "  val_correct=0\n",
        "  val_precision=0\n",
        "  totalVaLoss=0\n",
        "  totalValAcc=0\n",
        "  for j, valdata in enumerate(valoader):\n",
        "    model.eval()\n",
        "    valdata=valdata.to(device)\n",
        "\n",
        "    if args.laplace==True:\n",
        "      valfeats=valdata.laplace\n",
        "    else:\n",
        "      valfeats=valdata.x.float()\n",
        "\n",
        "    val_classify,val_atts,val_fuser=model(valfeats,valdata.edge_index,valdata.batch,weights,valdata.batch.unique().shape[0],device,valdata,pretrain=args.pretrain)\n",
        "\n",
        "    val_loss= criterion(val_classify[valdata.mask], valdata.y)\n",
        "\n",
        "    totalVaLoss+=val_loss\n",
        "\n",
        "    val_pred = val_classify.argmax(dim=1)\n",
        "    #print(val_pred)\n",
        "    val_correct = val_pred[valdata.mask] == valdata.y\n",
        "    val_acc = int(val_correct.sum()) / int(valdata.mask.sum())\n",
        "\n",
        "    totalValAcc+=val_acc\n",
        "\n",
        "  totalValAcc/=(j+1)\n",
        "\n",
        "  if val_acc>=temp:\n",
        "    temp=val_acc\n",
        "    when=epoch\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'best_weights':weights,\n",
        "          }, './'+args.webdata+'_'+str(epoch)+'.pth')\n",
        "\n",
        "  #lplot.append(totalLoss)\n",
        "  #vplot.append(totalVaLoss)\n",
        "\n",
        "  print(f'Epoch: {epoch:03d}, Loss: {loss.item():.4f},Train Acc: {totalAcc:.4f}, Val_Loss: {val_loss.item():.4f},Val Acc: {totalValAcc:.4f}')\n",
        "# plt.plot(torch.stack(lplot).detach().cpu().numpy(),label=\"training\")\n",
        "# plt.plot(torch.stack(vplot).detach().cpu().numpy(),label=\"validation\")\n",
        "# plt.legend(loc=\"upper left\")\n",
        "\n",
        "#from torchmetrics import AUROC\n",
        "#auroc = AUROC(task=\"binary\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84Ny3R0oAvfQ",
        "outputId": "201bd0ce-ec8e-4a8a-e91f-27125967766f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z54yU73omBcp",
        "outputId": "97a2c8f4-3b4a-4b3b-a57d-b796dbcd0f8e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classify[data.mask].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "-oaOpzFbA1zT",
        "outputId": "538437eb-9e63-43e0-f5be-abaab588b46e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "The shape of the mask [1920] at index 0 does not match the shape of the indexed tensor [32, 5] at index 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7f4c4e889061>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassify\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [1920] at index 0 does not match the shape of the indexed tensor [32, 5] at index 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=\"cuda\"\n",
        "checkpoint = torch.load('./'+args.webdata+'_'+str(when)+'.pth')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "for state in optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.to(device)\n",
        "\n",
        "weights2=checkpoint['best_weights']\n",
        "weights2=weights2.to(device)"
      ],
      "metadata": {
        "id": "TbXkEX6BliNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_correct=0\n",
        "\n",
        "for j, testdata in enumerate(testloader):\n",
        "  model.eval()\n",
        "  testdata=testdata.to(device)\n",
        "\n",
        "  if args.laplace==True:\n",
        "    testfeats=testdata.laplace\n",
        "  else:\n",
        "    testfeats=testdata.x.float()\n",
        "\n",
        "  test_classify,val_atts,val_fuser=model(testfeats,testdata.edge_index,testdata.batch,weights2,testdata.batch.unique().shape[0],device,testdata,pretrain=args.pretrain)\n",
        "\n",
        "  test_pred = test_classify.argmax(dim=1)\n",
        "  #print(val_pred)\n",
        "  test_correct = test_pred[testdata.mask] == testdata.y\n",
        "  test_acc = int(test_correct.sum()) / int(testdata.mask.sum())\n",
        "\n"
      ],
      "metadata": {
        "id": "4OWTcwjik_yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BOGqD-pllsm",
        "outputId": "e65abd05-d37b-4b74-cec7-a412833ae50f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3aRoy_kPsILO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}